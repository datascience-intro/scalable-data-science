// Databricks notebook source
// MAGIC %md
// MAGIC ScaDaMaLe Course [site](https://lamastex.github.io/scalable-data-science/sds/3/x/) and [book](https://lamastex.github.io/ScaDaMaLe/index.html)

// COMMAND ----------

// MAGIC %md 
// MAGIC # Robotics Dataset
// MAGIC 
// MAGIC The estimation of probability density functions in a configuration space is of fundamental importance for many applications in probabilistic robotics and sampling-based robot motion planning. 
// MAGIC 
// MAGIC Why is this application relevant? 
// MAGIC Standard sampling-based motion planners such as RRTConnect randomly explore the search space to iteratively build up a solution path. To speed up the planning it is common to use heuristics to explore the space in an informed way. The estimated densities are useful for designing such heurisitics. High-density regions might for example indicate narrow passages in the search space. 
// MAGIC 
// MAGIC In the following we are going to apply the previously introduced algorithm to estimate densities of points in the configuration space of a multi-joint articulated robot arm. As shown in the figure below, the robot consists of 10 rotational joints and is placed in a 2-dim workspace. Several workspace obstacles are placed in the scene. 
// MAGIC 
// MAGIC We ran RRT-Connect (a well-known shortest path motion planner) for different intial and goal configuration of the robot. The robot always starts in the right half of the workspace while the goal lies within one of the narrow passages as depicted in the figure.
// MAGIC 
// MAGIC To generate a dataset, we stitched all configurations from all planned paths together to one collection of joint configurations. Our goal is to estimate the density of robot configurations as generated by the RRTConnect motion planner. Originally, we used a dataset of ~125k points in 10 dimensions. Unfortunately, we observed that the current implementation does not scale well to such large dataset. For the scope of this project we instead run the method for 1000 points. Scaling up the implementation to larger dataset will be done in future work.  
// MAGIC 
// MAGIC <img src="files/group17/images/scene.png"/>
// MAGIC 
// MAGIC Here are some examples of paths found by the planner (red -> final configuration, black -> initial configuration, green-> intermediate confgurations along the path):
// MAGIC 
// MAGIC <img src="files/group17/images/path_test_0.png"/>
// MAGIC 
// MAGIC <img src="files/group17/images/path_test_1.png"/>
// MAGIC 
// MAGIC <img src="files/group17/images/path_test_2.png"/>

// COMMAND ----------

// MAGIC %python
// MAGIC # # Read data from file
// MAGIC # joint_centers_all = spark.read.format("csv").load("dbfs:/FileStore/shared_uploads/robert.gieselmann@gmail.com/robotics_dataset_joint_centers.csv",inferSchema =True,header=False)
// MAGIC # joint_centers_to_plot = np.reshape(np.array(joint_centers_all.collect()), (-1, 2))
// MAGIC 
// MAGIC # # Plot 2d histogram of joint center positions
// MAGIC # f = plt.figure()
// MAGIC # plt.hist2d(joint_centers_to_plot[:,0], joint_centers_to_plot[:,1], bins=250)
// MAGIC # plt.xlabel("x")
// MAGIC # plt.ylabel("y")
// MAGIC # plt.title("Distribution of joint center coordinates")
// MAGIC 
// MAGIC # # Display figure
// MAGIC # display()

// COMMAND ----------

// MAGIC %python
// MAGIC #import os
// MAGIC #os.remove("/dbfs/FileStore/group17/data/robotics_test/joint_configs_test.csv")
// MAGIC #os.remove("/dbfs/FileStore/group17/data/robotics_train/joint_configs_train.csv")
// MAGIC #os.listdir("/dbfs/FileStore/group17/data/robotics_train")
// MAGIC #os.rename("/dbfs/FileStore/group17/data/robotics", "/dbfs/FileStore/group17/data/robotics_train")

// COMMAND ----------

import org.apache.spark.mllib.random.RandomRDDs
import org.apache.spark.mllib.feature.Normalizer
import org.apache.spark.rdd.RDD
import breeze.linalg._
import breeze.numerics._

// COMMAND ----------

// MAGIC %scala
// MAGIC // Constants
// MAGIC val N = 1000 // train size
// MAGIC val M = 100 // test size
// MAGIC val D = 10 // dimensionality
// MAGIC val T = 100 // number of rays
// MAGIC val one_vs_all = true

// COMMAND ----------

// Read the files from csv and convert to RDD
val df_train = spark.read.option("inferSchema", "true").option("header", "false").format("csv").load("/FileStore/group17/data/robotics_train/joint_configs_train.csv")
val df_test = spark.read.option("inferSchema", "true").option("header", "false").format("csv").load("/FileStore/group17/data/robotics_test/joint_configs_test.csv")

// COMMAND ----------

// Convert to RDD 
val rdd_train = df_train.rdd.map(_.toSeq.toArray.map(_.toString.toDouble))
val rdd_test = df_train.rdd.map(_.toSeq.toArray.map(_.toString.toDouble))

// Convert to Array[(Long, DenseVector)]
val train_data = rdd_train.zipWithIndex().map { case (v, i) => (i, new DenseVector(v.toArray)) }
val test_data = if(one_vs_all) train_data else rdd_test.zipWithIndex().map { case (v, i) => (i, new DenseVector(v.toArray)) }

// COMMAND ----------

train_data.collect

// COMMAND ----------

def get_uni_sphere() = {
  var u = RandomRDDs.normalVectorRDD(sc, T, D)
  u = new Normalizer().transform(u)
  var t = u.zipWithIndex().map { case (v, i) => (i, new DenseVector(v.toArray)) }
  t
}
  
val rays = get_uni_sphere()

// COMMAND ----------

def compute_dst_sq() = { // (N, M)
  // dst[n, m] = |x_n - x'_m|^2
  val dst = train_data.cartesian(test_data).map { case ((n, train_vec), (m, test_vec)) => ((n, m), sum(((train_vec - test_vec) *:* (train_vec - test_vec)) ^:^ 2.0) ) }
  dst
}

def compute_pu(data: RDD[(Long, DenseVector[Double])]) = { // (data.N, T)
  // pu[n, t] = <data_n, ray_t>
  val pu = data.cartesian(rays).map { case ((n, data_vec), (t, ray_vec)) => ((n, t), data_vec dot ray_vec) }
  pu
}

val dst = compute_dst_sq()
val pu_train = compute_pu(train_data)
val pu_test = compute_pu(test_data)

// COMMAND ----------

def compute_ray_lengths() = { // (M, T)
  // lengths[m, t, n] = dst[n, m] / (2 * (pu_train[n, t] - pu_test[m, t]))
  def compute_length(n: Long, m: Long, dst_val: Double, pu_train_val: Double, pu_test_val: Double) = {
    if (one_vs_all && n == m) {
      Double.PositiveInfinity
    } else {
      val res = dst_val / (2 * (pu_train_val - pu_test_val))
      if (res < 0) Double.PositiveInfinity else res
    }
  }
  
  def my_min(a: Double, b: Double) = {min(a, b)}
        
  val lengths = dst.cartesian(sc.range(0, T))
    .map { case (((n, m), dst_val), t) => ((n, t), (m, dst_val)) }  
    .join(pu_train) 
    .map { case ((n, t), ((m, dst_val), pu_train_val)) => ((m, t), (n, dst_val, pu_train_val)) }
    .join(pu_test) 
    .map { case ((m, t), ((n, dst_val, pu_train_val), pu_test_val)) => ((m, t), compute_length(n, m, dst_val, pu_train_val, pu_test_val)) } 
    .aggregateByKey(Double.PositiveInfinity)(my_min, my_min)  
  lengths
}

val lengths = compute_ray_lengths()


// COMMAND ----------

def compute_weights() = { // (M, )
  def agg_f(a: (Double, Double), b: (Double, Double)) = { (a._1 + b._1, a._2 + b._2) }
  
  val weights = lengths.map { case ((m, t), length) => (m, if (!length.isInfinity) (1.0, length) else (0.0, 0.0)) }
    .aggregateByKey((0.0, 0.0))(agg_f, agg_f)
    .map { case (m, (val1, val2)) => (m, if (val1 > 0) val1 / val2 else 0.0) }
  weights
}

val weights = compute_weights()

// COMMAND ----------

//def save_data(name: String, data: RDD[(Long, DenseVector[Double])]) = {
//  data.map { case (k, v) => k.toString() + "," + v.toArray.mkString(",")}
//    .toDF.repartition(1).write.format("csv").mode(SaveMode.Overwrite).option("quote", " ").save("dbfs:/FileStore/group17/data/" + name)
//}

def save_weights(name: String, data: RDD[(Long, Double)]) = {
  data.map { case (k, v) => k.toString() + "," + v.toString}
    .toDF.repartition(1).write.format("csv").mode(SaveMode.Overwrite).option("quote", " ").save("dbfs:/FileStore/group17/data/" + name)
}

save_weights("robotics_weights", weights)

// COMMAND ----------

weights.collect